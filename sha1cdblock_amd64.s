// Code generated by command: go run asm.go -out ../sha1cdblock_amd64.s -pkg sha1cd. DO NOT EDIT.

//go:build !noasm && gc && amd64 && !arm64

#include "textflag.h"

#define RoundConst0 $1518500249 // 0x5A827999
#define RoundConst1 $1859775393 // 0x6ED9EBA1
#define RoundConst2 $2400959708 // 0x8F1BBCDC
#define RoundConst3 $3395469782 // 0xCA62C1D6

// FUNC1 f = (b & c) | ((~b) & d)
#define FUNC1(b, c, d) \
	MOVL d, R15; \
	XORL c, R15; \
	ANDL b, R15; \
	XORL d, R15

// FUNC2 f = b ^ c ^ d
#define FUNC2(b, c, d) \
	MOVL b, R15; \
	XORL c, R15; \
	XORL d, R15

// FUNC3 f = (b & c) | (b & d) | (c & d)
#define FUNC3(b, c, d) \
	MOVL b, R15; \
	ORL c, R15; \
	ANDL d, R15; \
	MOVL b, R14; \
	ANDL c, R14; \
	ORL R14, R15

#define FUNC4(b, c, d) FUNC2(b, c, d)
	
#define MIX(a, b, c, d, e, k) \
	RORL $2, b; \
	ADDL R15, e; \
	MOVL R14, 68(SP); \
	MOVL a, R14; \
	RORL $27, R14; \
	ADDL k, e; \
	ADDL DI, e; \
	ADDL R14, e; \
	MOVL 68(SP), R14

#define LOAD(index) \
	MOVL (index*4)(SI), DI; \
	BSWAPL DI; \
	MOVL DI, (index*4)(SP)

#define LOADCS(a, b, c, d, e, index) \
	MOVL R15, 64(SP); \
	MOVQ cs_base+56(FP), R15; \
	MOVL a, ((index*20))(R15); \
	MOVL b, ((index*20)+4)(R15); \
	MOVL c, ((index*20)+8)(R15); \
	MOVL d, ((index*20)+12)(R15); \
	MOVL e, ((index*20)+16)(R15); \
	MOVL 64(SP), R15

#define SHUFFLE(index) \
	MOVL ((index&0xf)*4)(SP), DI; \
	MOVL (((index-3)&0xf)*4)(SP), R14; \
	XORL R14, DI; \
	MOVL (((index-8)&0xf)*4)(SP), R14; \
	XORL R14, DI; \
	MOVL (((index-14)&0xf)*4)(SP), R14; \
	XORL R14, DI; \
	RORL $31, DI; \
	MOVL DI, ((index&0xf)*4)(SP)

// LOADM1 stores message word to m1 array.
#define LOADM1(index) \
	MOVL R15, 72(SP); \
	MOVQ m1_base+32(FP), R15; \
	MOVL ((index&0xf)*4)(SP), DI; \
	MOVL DI, (index*4)(R15); \
	MOVL 72(SP), R15

#define ROUND1(a, b, c, d, e, index) \
	LOAD(index); \
	FUNC1(b, c, d); \
	MIX(a, b, c, d, e, RoundConst0); \
	LOADM1(index)

#define ROUND1x(a, b, c, d, e, index) \
	SHUFFLE(index); \
	FUNC1(b, c, d); \
	MIX(a, b, c, d, e, RoundConst0); \
	LOADM1(index)

#define ROUND2(a, b, c, d, e, index) \
	SHUFFLE(index); \
	FUNC2(b, c, d); \
	MIX(a, b, c, d, e, RoundConst1); \
	LOADM1(index)

#define ROUND3(a, b, c, d, e, index) \
	SHUFFLE(index); \
	FUNC3(b, c, d); \
	MIX(a, b, c, d, e, RoundConst2); \
	LOADM1(index)

#define ROUND4(a, b, c, d, e, index) \
	SHUFFLE(index); \
	FUNC4(b, c, d); \
	MIX(a, b, c, d, e, RoundConst3); \
	LOADM1(index)

// func blockAMD64(dig *digest, p []byte, m1 []uint32, cs [][5]uint32)
TEXT Â·blockAMD64(SB), NOSPLIT, $80-80
	MOVQ dig+0(FP), R8
	MOVQ p_base+8(FP), SI
	MOVQ p_len+16(FP), DX
	
	SHRQ $6, DX
	SHLQ $6, DX
	ADDQ SI, DX
	MOVQ DX, 76(SP)  // Save end pointer on stack

	// Load h0-h4 into R9-R13.
	MOVL (R8), R9                   // R9 = h0
	MOVL 4(R8), R10                 // R10 = h1
	MOVL 8(R8), R11                 // R11 = h2
	MOVL 12(R8), R12                // R12 = h3
	MOVL 16(R8), R13                // R13 = h4

loop:
	// len(p) >= chunk
	CMPQ SI, 76(SP)
	JNB end

	// Initialize working registers a, b, c, d, e.
	MOVL R9, AX
	MOVL R10, BX
	MOVL R11, CX
	MOVL R12, DX
	MOVL R13, BP

	// ROUND1 (steps 0-15)
	LOADCS(AX, BX, CX, DX, BP, 0)
	ROUND1(AX, BX, CX, DX, BP, 0)
	ROUND1(BP, AX, BX, CX, DX, 1)
	ROUND1(DX, BP, AX, BX, CX, 2)
	ROUND1(CX, DX, BP, AX, BX, 3)
	ROUND1(BX, CX, DX, BP, AX, 4)
	ROUND1(AX, BX, CX, DX, BP, 5)
	ROUND1(BP, AX, BX, CX, DX, 6)
	ROUND1(DX, BP, AX, BX, CX, 7)
	ROUND1(CX, DX, BP, AX, BX, 8)
	ROUND1(BX, CX, DX, BP, AX, 9)
	ROUND1(AX, BX, CX, DX, BP, 10)
	ROUND1(BP, AX, BX, CX, DX, 11)
	ROUND1(DX, BP, AX, BX, CX, 12)
	ROUND1(CX, DX, BP, AX, BX, 13)
	ROUND1(BX, CX, DX, BP, AX, 14)
	ROUND1(AX, BX, CX, DX, BP, 15)

	// ROUND1x (steps 16-19) - same as ROUND1 but with no data load.
	ROUND1x(BP, AX, BX, CX, DX, 16)
	ROUND1x(DX, BP, AX, BX, CX, 17)
	ROUND1x(CX, DX, BP, AX, BX, 18)
	ROUND1x(BX, CX, DX, BP, AX, 19)

	// ROUND2 (steps 20-39)
	ROUND2(AX, BX, CX, DX, BP, 20)
	ROUND2(BP, AX, BX, CX, DX, 21)
	ROUND2(DX, BP, AX, BX, CX, 22)
	ROUND2(CX, DX, BP, AX, BX, 23)
	ROUND2(BX, CX, DX, BP, AX, 24)
	ROUND2(AX, BX, CX, DX, BP, 25)
	ROUND2(BP, AX, BX, CX, DX, 26)
	ROUND2(DX, BP, AX, BX, CX, 27)
	ROUND2(CX, DX, BP, AX, BX, 28)
	ROUND2(BX, CX, DX, BP, AX, 29)
	ROUND2(AX, BX, CX, DX, BP, 30)
	ROUND2(BP, AX, BX, CX, DX, 31)
	ROUND2(DX, BP, AX, BX, CX, 32)
	ROUND2(CX, DX, BP, AX, BX, 33)
	ROUND2(BX, CX, DX, BP, AX, 34)
	ROUND2(AX, BX, CX, DX, BP, 35)
	ROUND2(BP, AX, BX, CX, DX, 36)
	ROUND2(DX, BP, AX, BX, CX, 37)
	ROUND2(CX, DX, BP, AX, BX, 38)
	ROUND2(BX, CX, DX, BP, AX, 39)

	// ROUND3 (steps 40-59)
	ROUND3(AX, BX, CX, DX, BP, 40)
	ROUND3(BP, AX, BX, CX, DX, 41)
	ROUND3(DX, BP, AX, BX, CX, 42)
	ROUND3(CX, DX, BP, AX, BX, 43)
	ROUND3(BX, CX, DX, BP, AX, 44)
	ROUND3(AX, BX, CX, DX, BP, 45)
	ROUND3(BP, AX, BX, CX, DX, 46)
	ROUND3(DX, BP, AX, BX, CX, 47)
	ROUND3(CX, DX, BP, AX, BX, 48)
	ROUND3(BX, CX, DX, BP, AX, 49)
	ROUND3(AX, BX, CX, DX, BP, 50)
	ROUND3(BP, AX, BX, CX, DX, 51)
	ROUND3(DX, BP, AX, BX, CX, 52)
	ROUND3(CX, DX, BP, AX, BX, 53)
	ROUND3(BX, CX, DX, BP, AX, 54)
	ROUND3(AX, BX, CX, DX, BP, 55)
	ROUND3(BP, AX, BX, CX, DX, 56)
	ROUND3(DX, BP, AX, BX, CX, 57)

	LOADCS(CX, DX, BP, AX, BX, 1)
	ROUND3(CX, DX, BP, AX, BX, 58)
	ROUND3(BX, CX, DX, BP, AX, 59)

	// ROUND4 (steps 60-79)
	ROUND4(AX, BX, CX, DX, BP, 60)
	ROUND4(BP, AX, BX, CX, DX, 61)
	ROUND4(DX, BP, AX, BX, CX, 62)
	ROUND4(CX, DX, BP, AX, BX, 63)
	ROUND4(BX, CX, DX, BP, AX, 64)

	LOADCS(AX, BX, CX, DX, BP, 2)
	ROUND4(AX, BX, CX, DX, BP, 65)
	ROUND4(BP, AX, BX, CX, DX, 66)
	ROUND4(DX, BP, AX, BX, CX, 67)
	ROUND4(CX, DX, BP, AX, BX, 68)
	ROUND4(BX, CX, DX, BP, AX, 69)
	ROUND4(AX, BX, CX, DX, BP, 70)
	ROUND4(BP, AX, BX, CX, DX, 71)
	ROUND4(DX, BP, AX, BX, CX, 72)
	ROUND4(CX, DX, BP, AX, BX, 73)
	ROUND4(BX, CX, DX, BP, AX, 74)
	ROUND4(AX, BX, CX, DX, BP, 75)
	ROUND4(BP, AX, BX, CX, DX, 76)
	ROUND4(DX, BP, AX, BX, CX, 77)
	ROUND4(CX, DX, BP, AX, BX, 78)
	ROUND4(BX, CX, DX, BP, AX, 79)

	// Add working registers to hash state.
	ADDL AX, R9
	ADDL BX, R10
	ADDL CX, R11
	ADDL DX, R12
	ADDL BP, R13

	ADDQ $64, SI
	JMP loop

end:
	MOVQ dig+0(FP), R8
	MOVL R9, (R8)
	MOVL R10, 4(R8)
	MOVL R11, 8(R8)
	MOVL R12, 12(R8)
	MOVL R13, 16(R8)
	RET
